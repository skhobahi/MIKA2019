{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2次関数への学習可能勾配法の適用\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/wadayama/MIKA2019/blob/master/quadratic.ipynb)\n",
    "\n",
    "学習型勾配法のサンプルコードです。単純な2変数2次関数を目的関数としています。ステップサイズを固定した通常の勾配法と比較しています。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ライブラリ類のインポート"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import math\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## グローバル定数の設定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_val = 0.01 #学習可能スッテプサイズパラメータの初期値\n",
    "itr = 5 # 勾配法の反復数\n",
    "bs = 50 # mini batch size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 目的関数の定義\n",
    "2次関数\n",
    "$$\n",
    "f(x_1, x_2) = x_1^2 + 8 x_2^2\n",
    "$$\n",
    "をここでは考える。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 条件数が大きい二次関数を考える\n",
    "q = 8.0\n",
    "def f(x):\n",
    "    return x[:,0]**2 + q * x[:, 1]**2 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 勾配ベクトルの計算（数値微分を利用)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_numerical_f(x, bs):\n",
    "    eps = 1e-5\n",
    "    ret = torch.tensor([[0.0  , 0.0]]).repeat(bs,1)\n",
    "    h1  = torch.tensor([[eps, 0.0]]).repeat(bs,1)\n",
    "    h2  = torch.tensor([[0.0,   eps]]).repeat(bs,1)\n",
    "    ret[:,0] = (f(x+h1) - f(x))/eps\n",
    "    ret[:,1] = (f(x+h2) - f(x))/eps\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TGD クラス (Trainable Gradient Descent)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TGD(nn.Module):\n",
    "    def __init__(self, num_itr):\n",
    "        super(TGD, self).__init__()\n",
    "        self.beta = nn.Parameter(init_val*torch.ones(num_itr)) #学習可能ステップサイズパラメータ\n",
    "    def forward(self, num_itr, bs):\n",
    "        s = (torch.rand(bs, 2)*20.0 - 10.0) # ランダムな初期探索点\n",
    "        for i in range(num_itr):\n",
    "            s = s - self.beta[i] * grad_numerical_f(s, bs)\n",
    "        return s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 訓練ループ(インクリメンタルトレーニング）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TGD(itr)\n",
    "opt   = optim.Adam(model.parameters(), lr=0.001)\n",
    "loss_func = nn.MSELoss()\n",
    "solution = torch.tensor([[0.0, 0.0]]).repeat(bs,1) #解\n",
    "for gen in range(itr):\n",
    "    for i in range(1000):\n",
    "        opt.zero_grad()\n",
    "        x_hat = model(gen + 1, bs)\n",
    "        loss  = loss_func(x_hat, solution)\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "    print(gen, loss.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 学習しない普通の勾配法の準備"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GD(nn.Module):\n",
    "    def __init__(self, num_itr):\n",
    "        super(GD, self).__init__()\n",
    "    def forward(self, num_itr, bs, gamma):\n",
    "        s = (torch.rand(bs, 2)*20.0 - 10.0) # ランダムな初期点を設定\n",
    "        for i in range(num_itr):\n",
    "            s = s - gamma * grad_numerical_f(s, bs) # ステップサイズは固定\n",
    "        return s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 誤差値のプロット"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gd_model = GD(itr)\n",
    "## trained TGD model\n",
    "bs = 10000\n",
    "solution = torch.tensor([[0.0, 0.0]]).repeat(bs,1) #解\n",
    "with torch.no_grad():\n",
    "    for i in range(1): \n",
    "        norm_list = []\n",
    "        itr_list = []\n",
    "        for i in range(itr):\n",
    "            s_hat = model(i, bs)\n",
    "            err = (torch.norm(solution - s_hat)**2).item()/bs\n",
    "            norm_list.append(math.log10(err))\n",
    "            itr_list.append(i)\n",
    "        plt.plot(itr_list, norm_list, color=\"red\", label=\"TGD\",marker='o')\n",
    "## normal GD\n",
    "gamma = 0.08\n",
    "norm_list = []\n",
    "itr_list = []\n",
    "for i in range(itr):\n",
    "    s_hat = gd_model(i, bs, gamma)\n",
    "    err = (torch.norm(solution - s_hat)**2).item()/bs\n",
    "    norm_list.append(math.log10(err))\n",
    "    itr_list.append(i)\n",
    "plt.plot(itr_list, norm_list, color=\"green\", label=\"GD, gamma = \" + str(gamma),marker='o')\n",
    "gamma = 0.10\n",
    "norm_list = []\n",
    "itr_list = []\n",
    "for i in range(itr):\n",
    "    s_hat = gd_model(i, bs, gamma)\n",
    "    err = (torch.norm(solution - s_hat)**2).item()/bs\n",
    "    norm_list.append(math.log10(err))\n",
    "    itr_list.append(i)\n",
    "plt.plot(itr_list, norm_list, color=\"orange\", label=\"GD, gamma = \" + str(gamma),marker='o')\n",
    "gamma = 0.12\n",
    "norm_list = []\n",
    "itr_list = []\n",
    "for i in range(itr):\n",
    "    s_hat = gd_model(i, bs, gamma)\n",
    "    err = (torch.norm(solution - s_hat)**2).item()/bs\n",
    "    norm_list.append(math.log10(err))\n",
    "    itr_list.append(i)\n",
    "plt.plot(itr_list, norm_list, color=\"blue\", label=\"GD, gamma = \" + str(gamma),marker='o')\n",
    "#plt.title(\"Error curves\")\n",
    "plt.grid()\n",
    "plt.xlabel(\"iteration\")\n",
    "plt.ylabel(\"log10 of squared error\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $\\beta_t$の学習結果をプロットしてみる"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = model.beta.to(\"cpu\")\n",
    "gval = g.detach().numpy()\n",
    "gval = gval[0:itr]\n",
    "ind = np.linspace(0,itr-1,itr)\n",
    "plt.plot(ind, gval,marker='o')\n",
    "plt.xlabel(\"index t\")\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
